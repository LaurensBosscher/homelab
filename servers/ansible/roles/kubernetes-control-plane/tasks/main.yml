---
- name: Get Tailscale IP address
  ansible.builtin.shell: ip -4 addr show tailscale0 | grep -oP '(?<=inet\s)\d+(\.\d+){3}'
  register: tailscale_ip
  changed_when: false
  failed_when: tailscale_ip.stdout == ""

- name: Get eth0 IP address
  ansible.builtin.shell: ip -4 addr show eth0 | grep -oP '(?<=inet\s)\d+(\.\d+){3}'
  register: eth0_ip
  changed_when: false
  failed_when: eth0_ip.stdout == ""

- name: Ensure /etc/rancher/k3s directory exists
  ansible.builtin.file:
    path: /etc/rancher/k3s
    state: directory
    mode: "0755"

- name: Create /etc/rancher/k3s/config.yaml with built-in Tailscale configuration
  ansible.builtin.copy:
    content: |
      flannel-backend: wireguard-native
      flannel-external-ip: true
      flannel-iface: eth0

      prefer-bundled-bin: true

      node-external-ip: {{ eth0_ip.stdout }}  # Advertises eth0 IP for external access

      # Swap support for ZRAM, this allows containers to spike above the configured memory limits safely
      # Since we're using ZRAM the performance impact is negligible for spikes. 
      # We should monitor the pods and ensure that they're not constantly swapping.
      failSwapOn: false

      # Disable Traefik (use your own ingress controller)
      disable: traefik

      # Enable secret encryption at rest
      secrets-encryption: true

      # etcd tunings for high-latency (in milliseconds as integers)
      etcd-arg:
        - "heartbeat-interval=500"  # 500ms
        - "election-timeout=5000"   # 5 seconds = 5000ms
        - 'log-level=error' # Since etcd logs end-up mixed with the K3S logs make it less verbose
        - 'warning-apply-duration:1000ms'

      # Wide cluster optimizations
      kube-apiserver-arg:
        - "default-not-ready-toleration-seconds=30"
        - "default-unreachable-toleration-seconds=30"

      kube-controller-manager-arg:
        - "node-monitor-period=10s"
        - "node-monitor-grace-period=30s"

      kubelet-arg:
        - "node-status-update-frequency=10s"
        - "max-pods=200"  # Increase pod density for large clusters
    dest: /etc/rancher/k3s/config.yaml
    mode: "0644"

- name: Configure kubelet to use limited swap and resource constraints
  ansible.builtin.blockinfile:
    create: True
    path: /var/lib/rancher/k3s/agent/etc/kubelet.conf.d/01-swap-and-constraints.conf
    block: |
      kind: KubeletConfiguration
      apiVersion: kubelet.config.k8s.io/v1beta1

      memorySwap:
        swapBehavior: LimitedSwap
      systemReserved:
        cpu: 100m
        memory: 256Mi
      kubeReserved:
        cpu: 100m
        memory: 256Mi
    marker: "# {mark} ANSIBLE MANAGED BLOCK - memorySwap"
    mode: "0644"
